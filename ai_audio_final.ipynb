{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGUsXnHogXRf"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "KXKLqjwKhXkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"yousirui1/fsd50k\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "metadata": {
        "id": "RBPRw9vfoBKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: inspect the directory structure\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Root path from kagglehub:\", path)\n",
        "print(\"Contents:\")\n",
        "print(os.listdir(path))\n"
      ],
      "metadata": {
        "id": "_jUyCSWsouRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "root_dir = os.path.join(path, \"fsd50k\")\n",
        "\n",
        "print(\"Inside fsd50k folder:\")\n",
        "print(os.listdir(root_dir))"
      ],
      "metadata": {
        "id": "lzLDsY59yZ8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "root_dir = os.path.join(path, \"fsd50k\")\n",
        "\n",
        "DEV_AUDIO_DIR = os.path.join(root_dir, \"FSD50K.dev_audio_16k\")\n",
        "EVAL_AUDIO_DIR = os.path.join(root_dir, \"FSD50K.eval_audio_16k\")\n",
        "GROUND_TRUTH_DIR = os.path.join(root_dir, \"FSD50K.ground_truth\")\n",
        "METADATA_DIR = os.path.join(root_dir, \"FSD50K.metadata\")\n",
        "\n",
        "print(\"DEV AUDIO:\", DEV_AUDIO_DIR)\n",
        "print(\"EVAL AUDIO:\", EVAL_AUDIO_DIR)\n",
        "print(\"GROUND TRUTH:\", GROUND_TRUTH_DIR)\n",
        "print(\"METADATA:\", METADATA_DIR)\n",
        "\n",
        "print(\"\\nDEV audio sample:\", os.listdir(DEV_AUDIO_DIR)[:5])\n",
        "print(\"GT files:\", os.listdir(GROUND_TRUTH_DIR))\n",
        "print(\"Metadata files:\", os.listdir(METADATA_DIR))"
      ],
      "metadata": {
        "id": "a3YMsqa_yySb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load DEV ground truth CSV\n",
        "for f in os.listdir(GROUND_TRUTH_DIR):\n",
        "    if \"dev\" in f.lower() and f.endswith(\".csv\"):\n",
        "        DEV_GT_PATH = os.path.join(GROUND_TRUTH_DIR, f)\n",
        "        break\n",
        "\n",
        "print(\"Using GT file:\", DEV_GT_PATH)\n",
        "\n",
        "dev_gt = pd.read_csv(DEV_GT_PATH)\n",
        "print(\"Total Dev Labels:\", len(dev_gt))\n",
        "dev_gt.head()\n"
      ],
      "metadata": {
        "id": "y5ienumZzimb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check column names\n",
        "print(\"Columns:\", dev_gt.columns)\n"
      ],
      "metadata": {
        "id": "OYnbkPhf2__w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: auto-detect filename column\n",
        "\n",
        "possible_cols = [\"fname\", \"filename\", \"file_name\", \"clip_id\", \"id\", \"audio_id\"]\n",
        "file_col = None\n",
        "\n",
        "for col in dev_gt.columns:\n",
        "    if col.lower() in possible_cols:\n",
        "        file_col = col\n",
        "        break\n",
        "\n",
        "# If not matched directly, pick the first string/object column\n",
        "if file_col is None:\n",
        "    for col in dev_gt.columns:\n",
        "        if dev_gt[col].dtype == \"object\":\n",
        "            file_col = col\n",
        "            break\n",
        "\n",
        "print(\"Detected filename column:\", file_col)\n"
      ],
      "metadata": {
        "id": "Z5NiE4E960FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: build full file paths\n",
        "\n",
        "dev_gt[\"file_path\"] = dev_gt[file_col].apply(\n",
        "    lambda x: os.path.join(DEV_AUDIO_DIR, str(x) + \".wav\")\n",
        ")\n",
        "\n",
        "# Check if files exist\n",
        "print(\"First 5 paths:\")\n",
        "print(dev_gt[\"file_path\"].head())\n",
        "\n",
        "print(\"\\nFile exists check for first sample:\",\n",
        "      os.path.exists(dev_gt[\"file_path\"].iloc[0]))"
      ],
      "metadata": {
        "id": "wi5efCds69L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# STEP 8: Load metadata and merge to get class names\n",
        "\n",
        "meta_files = os.listdir(METADATA_DIR)\n",
        "print(\"Metadata files:\", meta_files)\n",
        "\n",
        "# find class label metadata\n",
        "for f in meta_files:\n",
        "    if \"class\" in f.lower() or \"label\" in f.lower():\n",
        "        META_PATH = os.path.join(METADATA_DIR, f)\n",
        "        break\n",
        "\n",
        "print(\"Using metadata file:\", META_PATH)\n",
        "\n",
        "# Changed to pd.read_json as the file is a .json file\n",
        "meta_df = pd.read_json(META_PATH)\n",
        "print(\"Metadata columns:\", meta_df.columns)\n",
        "meta_df.head()"
      ],
      "metadata": {
        "id": "YqTnk3RH7ARx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try common key-based merge\n",
        "common_cols = list(set(dev_gt.columns).intersection(set(meta_df.columns)))\n",
        "print(\"Common columns:\", common_cols)\n"
      ],
      "metadata": {
        "id": "ENk06MQd7Xlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Transform meta_df and dev_gt for merging and perform the merge\n",
        "\n",
        "# 1. Transpose meta_df and reset index to make MIDs a column\n",
        "meta_df_transposed = meta_df.T.reset_index()\n",
        "meta_df_transposed = meta_df_transposed.rename(columns={'index': 'mid'})\n",
        "\n",
        "print(\"Transposed Metadata (first 5 rows):\")\n",
        "display(meta_df_transposed.head())\n",
        "\n",
        "# 2. Expand dev_gt by splitting 'mids' and exploding\n",
        "# First, convert 'mids' to string type to handle potential non-string entries safely\n",
        "dev_gt_expanded = dev_gt.assign(mids=dev_gt['mids'].astype(str).str.split(',')).explode('mids')\n",
        "\n",
        "print(\"\\nExpanded Dev Ground Truth (first 5 rows):\")\n",
        "display(dev_gt_expanded.head())\n",
        "\n",
        "# 3. Merge the expanded dev_gt with the transposed meta_df\n",
        "real_df = pd.merge(dev_gt_expanded, meta_df_transposed, left_on='mids', right_on='mid', how='left')\n",
        "\n",
        "print(f\"\\nMerged rows: {len(real_df)}\")\n",
        "display(real_df.head())"
      ],
      "metadata": {
        "id": "qJxI48f97Zld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Mark as REAL (1)\n",
        "\n",
        "real_df[\"label\"] = 1   # 1 = REAL, 0 = FAKE\n",
        "\n",
        "# Keep only what we need\n",
        "real_df = real_df[[\"file_path\", \"label\"] + list(real_df.columns[1:3])]\n",
        "real_df.head()\n"
      ],
      "metadata": {
        "id": "fv3g30rl7tqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10: Select diverse real sound classes automatically\n",
        "\n",
        "# Get most frequent classes\n",
        "class_col = real_df.columns[2]   # auto detect class column\n",
        "top_classes = real_df[class_col].value_counts().head(10).index.tolist()\n",
        "\n",
        "subset_real_df = real_df[real_df[class_col].isin(top_classes)]\n",
        "print(\"Selected classes:\", top_classes)\n",
        "print(\"Subset size:\", len(subset_real_df))\n"
      ],
      "metadata": {
        "id": "3FqetZET7xI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_real_csv = \"/content/fsd50k_real_subset.csv\"\n",
        "subset_real_df.to_csv(subset_real_csv, index=False)\n",
        "print(\"Saved REAL subset CSV to:\", subset_real_csv)\n",
        "\n",
        "subset_real_df.head()\n"
      ],
      "metadata": {
        "id": "uLdAPvBQ76jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q diffusers transformers accelerate soundfile torchaudio librosa\n"
      ],
      "metadata": {
        "id": "pzIsejrV7_rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import AudioLDM2Pipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "pipe = AudioLDM2Pipeline.from_pretrained(\n",
        "    \"cvssp/audioldm2\",\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "wlJJAI6w8VH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "real_df = pd.read_csv(\"/content/fsd50k_real_subset.csv\")\n",
        "print(\"Real samples:\", len(real_df))\n",
        "real_df.head()\n"
      ],
      "metadata": {
        "id": "HRB1tPGF8cDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_COL = real_df.columns[2]  # auto-detected earlier\n",
        "classes = real_df[CLASS_COL].unique().tolist()\n",
        "print(\"Classes selected:\", classes)\n"
      ],
      "metadata": {
        "id": "YGicxL6eIDjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import AudioLDMPipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "pipe = AudioLDMPipeline.from_pretrained(\n",
        "    \"cvssp/audioldm\",\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "GWpvk5bOsr9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "FAKE_ROOT = \"/content/fake_audio\"\n",
        "os.makedirs(FAKE_ROOT, exist_ok=True)\n",
        "\n",
        "print(\"Fake audio root:\", FAKE_ROOT)\n",
        "\n"
      ],
      "metadata": {
        "id": "g1_rQ3x6ssMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import re\n",
        "\n",
        "clips_per_class = 5   # you can increase later (10â€“20 for paper)\n",
        "\n",
        "for cls in classes:\n",
        "    cls_clean = cls.replace(\" \", \"_\")\n",
        "    cls_dir = os.path.join(FAKE_ROOT, cls_clean)\n",
        "    os.makedirs(cls_dir, exist_ok=True)\n",
        "\n",
        "    prompt = f\"realistic {cls.replace('_',' ')} environmental sound\"\n",
        "    print(f\"\\nGenerating FAKE for class: {cls}\")\n",
        "\n",
        "    for i in range(clips_per_class):\n",
        "        with torch.no_grad():\n",
        "            output = pipe(\n",
        "                prompt,\n",
        "                num_inference_steps=30,\n",
        "                audio_length_in_s=5.0\n",
        "            )\n",
        "\n",
        "        audio = output.audios[0]\n",
        "\n",
        "        fname = f\"fake_{cls_clean}_{i}.wav\"\n",
        "        save_path = os.path.join(cls_dir, fname)\n",
        "\n",
        "        sf.write(save_path, audio, 16000)\n",
        "        print(\"Saved:\", save_path)\n"
      ],
      "metadata": {
        "id": "TpUDDfhjssQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "fake_rows = []\n",
        "\n",
        "for cls in classes:\n",
        "    cls_clean = cls.replace(\" \", \"_\")\n",
        "    cls_dir = os.path.join(FAKE_ROOT, cls_clean)\n",
        "\n",
        "    for fname in os.listdir(cls_dir):\n",
        "        fake_rows.append({\n",
        "            \"file_path\": os.path.join(cls_dir, fname),\n",
        "            \"label\": 0,              # 0 = FAKE\n",
        "            CLASS_COL: cls\n",
        "        })\n",
        "\n",
        "fake_df = pd.DataFrame(fake_rows)\n",
        "\n",
        "print(\"Total FAKE samples:\", len(fake_df))\n",
        "fake_df.head()\n"
      ],
      "metadata": {
        "id": "od35GbeJtsAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.concat([real_df, fake_df], ignore_index=True)\n",
        "final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"FINAL dataset size:\", len(final_df))\n",
        "print(final_df[\"label\"].value_counts())\n",
        "\n",
        "final_csv_path = \"/content/real_vs_fake_fsd50k.csv\"\n",
        "final_df.to_csv(final_csv_path, index=False)\n",
        "\n",
        "print(\"Saved final dataset to:\", final_csv_path)\n"
      ],
      "metadata": {
        "id": "7s8LhwGGt0Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_p = [p for t, p in zip(all_labels, probs) if t==0]\n",
        "real_p = [p for t, p in zip(all_labels, probs) if t==1]\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(fake_p, bins=30, alpha=0.7, label=\"FAKE\")\n",
        "plt.hist(real_p, bins=30, alpha=0.7, label=\"REAL\")\n",
        "plt.title(\"Prediction Confidence Distribution\")\n",
        "plt.legend(); plt.show()\n"
      ],
      "metadata": {
        "id": "tHNq6vvUt79Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "063528fb"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "all_labels = []\n",
        "probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (xb, yb) in enumerate(val_loader):\n",
        "        # Handle cases where custom_collate_fn returns None for the entire batch\n",
        "        if xb is None or yb is None:\n",
        "            print(f\"Skipping batch {i} due to None values from custom_collate_fn.\")\n",
        "            continue\n",
        "\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = model(xb)\n",
        "            # Apply softmax to get probabilities for each class\n",
        "            batch_probs = F.softmax(out, dim=1)\n",
        "            # We are interested in the probability of being REAL (class 1)\n",
        "            batch_probs_real = batch_probs[:, 1].cpu().numpy()\n",
        "            batch_labels = yb.cpu().numpy()\n",
        "\n",
        "            # Ensure consistent lengths before extending\n",
        "            if len(batch_probs_real) != len(batch_labels):\n",
        "                print(f\"WARNING: Inconsistent lengths in batch {i}! Probs len: {len(batch_probs_real)}, Labels len: {len(batch_labels)}. Skipping this batch.\")\n",
        "                continue\n",
        "\n",
        "            probs.extend(batch_probs_real)\n",
        "            all_labels.extend(batch_labels)\n",
        "\n",
        "# Convert to numpy arrays for easier filtering\n",
        "all_labels = np.array(all_labels)\n",
        "probs = np.array(probs)\n",
        "\n",
        "# Separate probabilities for fake and real samples\n",
        "# Ensure both arrays are not empty before processing for histogram\n",
        "if len(all_labels) == 0 or len(probs) == 0:\n",
        "    print(\"WARNING: No valid samples collected for plotting histogram. Please check data loading.\")\n",
        "else:\n",
        "    fake_p = [p for t, p in zip(all_labels, probs) if t == 0]\n",
        "    real_p = [p for t, p in zip(all_labels, probs) if t == 1]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(fake_p, bins=30, alpha=0.7, label=\"FAKE predictions\")\n",
        "    plt.hist(real_p, bins=30, alpha=0.7, label=\"REAL predictions\")\n",
        "    plt.title(\"Prediction Confidence Distribution\")\n",
        "    plt.xlabel(\"Probability of being REAL\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the merged REAL + FAKE dataset\n",
        "final_csv_path = \"/content/real_vs_fake_fsd50k.csv\"   # change if different\n",
        "final_df = pd.read_csv(final_csv_path)\n",
        "\n",
        "print(\"Total samples:\", len(final_df))\n",
        "print(final_df[\"label\"].value_counts())   # 1 = REAL, 0 = FAKE\n",
        "final_df.head()\n"
      ],
      "metadata": {
        "id": "Cr3BHn5-0dQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Stratified split to keep real/fake balance\n",
        "train_df, val_df = train_test_split(\n",
        "    final_df,\n",
        "    test_size=0.2,\n",
        "    stratify=final_df[\"label\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Val size:\", len(val_df))\n",
        "print(\"Train label counts:\\n\", train_df[\"label\"].value_counts())\n",
        "print(\"Val label counts:\\n\", val_df[\"label\"].value_counts())\n"
      ],
      "metadata": {
        "id": "Fv6eK7jA14b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "SR = 16000          # sample rate\n",
        "N_MELS = 128        # Mel bands\n",
        "N_FFT = 1024        # FFT window\n",
        "HOP_LENGTH = 256    # hop\n",
        "FIXED_TIME_FRAMES = 128  # we will pad/crop to this many frames\n",
        "\n",
        "def extract_logmel(path):\n",
        "    y, sr = librosa.load(path, sr=SR)\n",
        "\n",
        "    # compute mel spectrogram\n",
        "    mel = librosa.feature.melspectrogram(\n",
        "        y=y,\n",
        "        sr=sr,\n",
        "        n_fft=N_FFT,\n",
        "        hop_length=HOP_LENGTH,\n",
        "        n_mels=N_MELS\n",
        "    )\n",
        "    logmel = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "    # time dimension = axis=1. We pad/crop to FIXED_TIME_FRAMES\n",
        "    if logmel.shape[1] < FIXED_TIME_FRAMES:\n",
        "        pad_width = FIXED_TIME_FRAMES - logmel.shape[1]\n",
        "        logmel = np.pad(logmel, ((0,0),(0,pad_width)), mode=\"constant\")\n",
        "    else:\n",
        "        logmel = logmel[:, :FIXED_TIME_FRAMES]\n",
        "\n",
        "    # shape = (N_MELS, FIXED_TIME_FRAMES)\n",
        "    return logmel\n"
      ],
      "metadata": {
        "id": "ivmnbTGA19_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class AntiFoleyDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        x = extract_logmel(row[\"file_path\"])   # (N_MELS, T)\n",
        "        x = torch.tensor(x).unsqueeze(0).float()  # (1, N_MELS, T)\n",
        "        y = torch.tensor(row[\"label\"]).long()    # 0 = fake, 1 = real\n",
        "        return x, y\n",
        "\n",
        "train_dataset = AntiFoleyDataset(train_df)\n",
        "val_dataset   = AntiFoleyDataset(val_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "len(train_dataset), len(val_dataset)\n"
      ],
      "metadata": {
        "id": "vIgTPUIx1-G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AntiFoleyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.apool = nn.AdaptiveAvgPool2d((8, 8))  # (C, 8, 8)\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
        "        self.fc2 = nn.Linear(256, 2)  # 2 classes: real / fake\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, 1, N_MELS, T)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "        x = self.apool(x)            # (B, 128, 8, 8)\n",
        "        x = x.view(x.size(0), -1)    # flatten\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AntiFoleyNet().to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "AkXzwMEz2HF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "4aZwWUiD2HKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# COMPLETE FINAL TRAINING CODE â€” AntiFoley++ (ONE CELL)\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import resample\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os # Import os for path handling in error messages\n",
        "\n",
        "# ================================\n",
        "# 1. DATASET (LOG-MEL)\n",
        "# ================================\n",
        "SR = 16000          # sample rate\n",
        "N_MELS = 128        # Mel bands\n",
        "N_FFT = 1024        # FFT window\n",
        "HOP_LENGTH = 256    # hop\n",
        "FIXED_TIME_FRAMES = 128  # we will pad/crop to this many frames\n",
        "\n",
        "class AntiFoleyDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        file_path = row[\"file_path\"]\n",
        "        try:\n",
        "            y, sr = librosa.load(file_path, sr=SR)\n",
        "\n",
        "            # compute mel spectrogram\n",
        "            mel = librosa.feature.melspectrogram(\n",
        "                y=y,\n",
        "                sr=sr,\n",
        "                n_fft=N_FFT,\n",
        "                hop_length=HOP_LENGTH,\n",
        "                n_mels=N_MELS\n",
        "            )\n",
        "            logmel = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "            # time dimension = axis=1. We pad/crop to FIXED_TIME_FRAMES\n",
        "            if logmel.shape[1] < FIXED_TIME_FRAMES:\n",
        "                pad_width = FIXED_TIME_FRAMES - logmel.shape[1]\n",
        "                logmel = np.pad(logmel, ((0,0),(0,pad_width)), mode=\"constant\")\n",
        "            else:\n",
        "                logmel = logmel[:, :FIXED_TIME_FRAMES]\n",
        "\n",
        "            x = torch.tensor(logmel).unsqueeze(0).float()  # (1, N_MELS, T)\n",
        "            y_label = torch.tensor(row[\"label\"]).long()    # 0 = fake, 1 = real\n",
        "\n",
        "            return x, y_label\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing file: {file_path}. Error: {e}\")\n",
        "            return None, None # Return None for problematic samples\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    # Filter out None samples\n",
        "    batch = [(x, y) for x, y in batch if x is not None and y is not None]\n",
        "    if not batch:\n",
        "        return None, None # Return None if the entire batch is invalid\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 2. BALANCE DATA (CRITICAL)\n",
        "# ================================\n",
        "real_df_bal = train_df[train_df[\"label\"] == 1]\n",
        "fake_df_bal = train_df[train_df[\"label\"] == 0]\n",
        "\n",
        "fake_df_bal = resample(fake_df_bal,\n",
        "                       replace=True,\n",
        "                       n_samples=len(real_df_bal),\n",
        "                       random_state=42)\n",
        "\n",
        "train_df_balanced = pd.concat([real_df_bal, fake_df_bal]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "train_dataset = AntiFoleyDataset(train_df_balanced)\n",
        "val_dataset   = AntiFoleyDataset(val_df)\n",
        "\n",
        "# ================================\n",
        "# 3. FAST DATALOADERS\n",
        "# ================================\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0, # Keeping at 0 for debugging. Set back to 4 later if stable.\n",
        "    pin_memory=True,\n",
        "    persistent_workers=False, # Set to False if num_workers is 0\n",
        "    collate_fn=custom_collate_fn # Use custom collate_fn to handle None samples\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0, # Keeping at 0 for debugging. Set back to 4 later if stable.\n",
        "    pin_memory=True,\n",
        "    persistent_workers=False, # Set to False if num_workers is 0\n",
        "    collate_fn=custom_collate_fn # Use custom collate_fn to handle None samples\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 4. AntiFoley++ MODEL (CNN + TRANSFORMER)\n",
        "# ================================\n",
        "class AntiFoleyPlus(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.apool = nn.AdaptiveAvgPool2d((16, 16))\n",
        "\n",
        "        self.embed = nn.Linear(128, 256)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=256, nhead=8, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.bn1(F.relu(self.conv1(x))))\n",
        "        x = self.pool(self.bn2(F.relu(self.conv2(x))))\n",
        "        x = self.pool(self.bn3(F.relu(self.conv3(x))))\n",
        "\n",
        "        x = self.apool(x)\n",
        "        x = x.mean(dim=2)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        x = self.embed(x)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        return self.fc2(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AntiFoleyPlus().to(device)\n",
        "\n",
        "# ================================\n",
        "# 5. FOCAL LOSS\n",
        "# ================================\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(reduction='none') # Change reduction to 'none'\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        ce_loss = self.ce(logits, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        return ((1 - pt) ** self.gamma * ce_loss).mean() # Apply mean after weighting\n",
        "\n",
        "criterion = FocalLoss(gamma=2.0)\n",
        "\n",
        "# ================================\n",
        "# 6. OPTIMIZER + SCHEDULER + AMP\n",
        "# ================================\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=2e-5, weight_decay=1e-3\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"min\", patience=3, factor=0.5\n",
        ")\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# ================================\n",
        "# 7. TRAINING WITH EARLY STOPPING\n",
        "# ================================\n",
        "EPOCHS = 1\n",
        "patience = 1\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "train_loss_list = [] # Initialize list to store training losses\n",
        "val_loss_list = []   # Initialize list to store validation losses\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    train_preds, train_labels = [], []\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        # Handle cases where custom_collate_fn returns None for the entire batch\n",
        "        if xb is None or yb is None:\n",
        "            continue\n",
        "\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        train_preds.extend(out.argmax(1).cpu().numpy())\n",
        "        train_labels.extend(yb.cpu().numpy())\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = accuracy_score(train_labels, train_preds)\n",
        "    train_loss_list.append(train_loss) # Store training loss\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_preds, val_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            # Handle cases where custom_collate_fn returns None for the entire batch\n",
        "            if xb is None or yb is None:\n",
        "                continue\n",
        "\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                out = model(xb)\n",
        "                loss = criterion(out, yb)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_preds.extend(out.argmax(1).cpu().numpy())\n",
        "            val_labels.extend(yb.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_loss_list.append(val_loss) # Store validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = deepcopy(model.state_dict())\n",
        "        print(\"âœ… New best model saved\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"ðŸ›‘ Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# ================================\n",
        "# 8. LOAD & SAVE BEST MODEL\n",
        "# ================================\n",
        "model.load_state_dict(best_model_state)\n",
        "torch.save(model.state_dict(), \"/content/antifoley_96_model.pth\")\n",
        "print(\"âœ… Final Best Model Saved\")"
      ],
      "metadata": {
        "id": "iwUsbLjp2HNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in val_loader:\n",
        "        xb = xb.to(device)\n",
        "        out = model(xb)\n",
        "        preds = out.argmax(dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(yb.numpy())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(\"Final Validation Accuracy:\", acc)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=[\"FAKE\", \"REAL\"]))\n"
      ],
      "metadata": {
        "id": "Ov0TgL6XV16G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}